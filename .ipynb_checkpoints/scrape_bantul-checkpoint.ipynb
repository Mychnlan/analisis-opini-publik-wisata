{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8294b748-9791-46c7-a19f-68cebb7319f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://travel.detik.com/domestic-destination/d-8108546/rekomendasi-5-desa-wisata-terbaik-di-bantul as detik\n",
      " -> got 25\n",
      "Scraping: https://www.detik.com/jogja/plesir/d-7089360/10-tempat-wisata-hits-di-bantul-selain-pantai-dari-alam-hingga-religi as detik\n",
      " -> got 27\n",
      "Scraping: https://jogja.idntimes.com/travel/destination/wisata-di-bantul-yang-lagi-hits-c1c2-01-llcyy-6nm29s as generic\n",
      " -> got 18\n",
      "Scraping: https://travel.kompas.com/read/2022/05/19/192720927/15-wisata-bantul-yogyakarta-dengan-pemandangan-alam-instagramable?page=all as generic\n",
      " -> got 101\n",
      "Scraping: https://alodiatour.com/blog/wisata-anak-bantul/ as generic\n",
      " -> got 42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 236\u001b[39m\n\u001b[32m    225\u001b[39m seed_urls = [\n\u001b[32m    226\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mhttps://travel.detik.com/domestic-destination/d-8108546/rekomendasi-5-desa-wisata-terbaik-di-bantul\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mdetik\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m    227\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mhttps://www.detik.com/jogja/plesir/d-7089360/10-tempat-wisata-hits-di-bantul-selain-pantai-dari-alam-hingga-religi\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mdetik\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mhttps://www.tripadvisor.co.id/Tourism-g2304084-Bantul_Yogyakarta_Region_Java-Vacations.html\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mtripadvisor\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m    233\u001b[39m ]\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# Run collection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m docs = \u001b[43mcollect_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_urls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTOTAL DOCUMENTS:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(docs))\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# If low amount, we will break article paragraphs further to increase doc count\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# Save raw\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 221\u001b[39m, in \u001b[36mcollect_all\u001b[39m\u001b[34m(seed_urls)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m -> got\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(docs))\n\u001b[32m    220\u001b[39m     all_docs.extend(docs)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPAUSE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_docs\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ------------------ Settings ------------------\n",
    "HEADLESS = True     # set False if you want to see browser\n",
    "MAX_TRIPADVISOR_REVIEWS = 120   # cap to avoid too-long runs\n",
    "PAUSE = 1.0         # polite delay between requests (seconds)\n",
    "OUTPUT_CSV = \"bantul_reviews.csv\"\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "def safe_get(url, headers=None, timeout=10):\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers or {\"User-Agent\":\"Mozilla/5.0\"}, timeout=timeout)\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        print(\"requests error:\", e)\n",
    "        return \"\"\n",
    "\n",
    "def split_paragraphs(text):\n",
    "    # split long article into paragraph-level docs\n",
    "    paras = [p.strip() for p in re.split(r'\\n+|\\r+|\\.\\s+', text) if len(p.strip())>40]\n",
    "    return paras\n",
    "\n",
    "# ------------------ Site-specific scrapers (Requests + BS4) ------------------\n",
    "\n",
    "def scrape_article_paragraphs(url, source_name=None, selector=None):\n",
    "    \"\"\"\n",
    "    Generic article scraper that returns list of dicts with paragraph-level docs.\n",
    "    If selector provided (CSS selector for main content), will use that; otherwise fallback to <p>.\n",
    "    \"\"\"\n",
    "    html = safe_get(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    title = soup.title.get_text(strip=True) if soup.title else \"\"\n",
    "    content = \"\"\n",
    "    if selector:\n",
    "        el = soup.select_one(selector)\n",
    "        if el:\n",
    "            content = \" \".join([p.get_text(\" \", strip=True) for p in el.find_all(\"p\")])\n",
    "    if not content:\n",
    "        paras = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "        content = \" \".join(paras)\n",
    "    docs = []\n",
    "    for para in split_paragraphs(content):\n",
    "        docs.append({\n",
    "            \"source\": source_name or re.sub(r'^https?://(www\\.)?','', url.split('/')[2]) if url else \"unknown\",\n",
    "            \"url\": url,\n",
    "            \"review_title\": title,\n",
    "            \"review_text\": para,\n",
    "            \"rating\": None,\n",
    "            \"date\": None\n",
    "        })\n",
    "    return docs\n",
    "\n",
    "# ------------------ Detik-specific (uses class \"detail__body-text\" often) ------------------\n",
    "def scrape_detik_article(url):\n",
    "    # Detik structure: try \"div.detail__body-text\" first\n",
    "    try:\n",
    "        html = safe_get(url)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"\"\n",
    "        content_div = soup.find(\"div\", class_=re.compile(\"detail__body-text|detail_text\"))\n",
    "        if content_div:\n",
    "            paras = [p.get_text(\" \", strip=True) for p in content_div.find_all(\"p\")]\n",
    "            docs = []\n",
    "            for p in paras:\n",
    "                if len(p) > 40:\n",
    "                    docs.append({\n",
    "                        \"source\":\"detik.com\",\n",
    "                        \"url\":url,\n",
    "                        \"review_title\": title,\n",
    "                        \"review_text\": p,\n",
    "                        \"rating\": None,\n",
    "                        \"date\": None\n",
    "                    })\n",
    "            return docs\n",
    "        else:\n",
    "            # fallback\n",
    "            return scrape_article_paragraphs(url, source_name=\"detik.com\")\n",
    "    except Exception as e:\n",
    "        print(\"detik scrape error:\", e)\n",
    "        return []\n",
    "\n",
    "# ------------------ IDN Times & Kompas & Alodiatour & Yogyes generic ------------------\n",
    "def scrape_generic_article(url):\n",
    "    # attempt to detect main article container heuristically\n",
    "    html = safe_get(url)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    title = soup.title.get_text(strip=True) if soup.title else \"\"\n",
    "    # common containers\n",
    "    cand = None\n",
    "    for cls in [\"read__content\", \"artikel-body\", \"article-content\", \"post-content\", \"entry-content\", \"td-post-content\"]:\n",
    "        cand = soup.find(\"div\", class_=re.compile(cls))\n",
    "        if cand:\n",
    "            break\n",
    "    if not cand:\n",
    "        # fallback to main\n",
    "        cand = soup.find(\"main\") or soup\n",
    "    paras = [p.get_text(\" \", strip=True) for p in cand.find_all(\"p\")]\n",
    "    docs = []\n",
    "    for p in paras:\n",
    "        if len(p) > 40:\n",
    "            docs.append({\n",
    "                \"source\": re.sub(r'^https?://(www\\.)?','', url.split('/')[2]),\n",
    "                \"url\": url,\n",
    "                \"review_title\": title,\n",
    "                \"review_text\": p,\n",
    "                \"rating\": None,\n",
    "                \"date\": None\n",
    "            })\n",
    "    return docs\n",
    "\n",
    "# ------------------ TripAdvisor (Selenium) ------------------\n",
    "def init_selenium(headless=True):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")  # gunakan mode headless baru\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--window-size=1200,800\")\n",
    "    chrome_options.add_argument(\"--lang=id-ID\")\n",
    "\n",
    "    # ✅ perbaikan utama di sini:\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def scrape_tripadvisor_city_reviews(city_url, max_reviews=100):\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    import time, re\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--lang=id-ID\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    driver.get(city_url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    collected = []\n",
    "    seen = set()\n",
    "\n",
    "    # scroll & collect\n",
    "    for _ in range(40):\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, \"div[data-automation='reviewCard']\")\n",
    "        for card in cards:\n",
    "            try:\n",
    "                text_el = card.find_element(By.CSS_SELECTOR, \"span[class*='QewHA']\")\n",
    "                text = text_el.text.strip()\n",
    "                if not text or text[:100] in seen:\n",
    "                    continue\n",
    "                seen.add(text[:100])\n",
    "                collected.append({\n",
    "                    \"source\": \"tripadvisor\",\n",
    "                    \"url\": driver.current_url,\n",
    "                    \"review_title\": \"\",\n",
    "                    \"review_text\": text,\n",
    "                    \"rating\": None,\n",
    "                    \"date\": None\n",
    "                })\n",
    "                if len(collected) >= max_reviews:\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        driver.execute_script(\"window.scrollBy(0, 800);\")\n",
    "        time.sleep(2)\n",
    "        if len(collected) >= max_reviews:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"✅ Ditemukan {len(collected)} ulasan TripAdvisor\")\n",
    "    return collected\n",
    "\n",
    "\n",
    "# ------------------ Main orchestration ------------------\n",
    "def collect_all(seed_urls):\n",
    "    \"\"\"\n",
    "    seed_urls: list of dicts: {\"url\":..., \"type\": \"detik|tripadvisor|generic\"}\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "    for s in seed_urls:\n",
    "        url = s[\"url\"]\n",
    "        typ = s.get(\"type\",\"generic\")\n",
    "        print(\"Scraping:\", url, \"as\", typ)\n",
    "        if typ == \"detik\":\n",
    "            docs = scrape_detik_article(url)\n",
    "        elif typ == \"tripadvisor\":\n",
    "            docs = scrape_tripadvisor_city_reviews(url, max_reviews=MAX_TRIPADVISOR_REVIEWS)\n",
    "        else:\n",
    "            docs = scrape_generic_article(url)\n",
    "        print(\" -> got\", len(docs))\n",
    "        all_docs.extend(docs)\n",
    "        time.sleep(PAUSE)\n",
    "    return all_docs\n",
    "\n",
    "# ------------------ Example seeds (add more) ------------------\n",
    "seed_urls = [\n",
    "    {\"url\":\"https://www.tripadvisor.co.id/Tourism-g2304084-Bantul_Yogyakarta_Region_Java-Vacations.html\", \"type\":\"tripadvisor\"}\n",
    "]\n",
    "\n",
    "# Run collection\n",
    "docs = collect_all(seed_urls)\n",
    "print(\"TOTAL DOCUMENTS:\", len(docs))\n",
    "\n",
    "# If low amount, we will break article paragraphs further to increase doc count\n",
    "# Save raw\n",
    "df = pd.DataFrame(docs)\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved raw to\", OUTPUT_CSV)\n",
    "\n",
    "# ------------------ Preprocessing ------------------\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# minimal stopwords (add more domain-specific if required)\n",
    "DEFAULT_STOPWORDS = set([\n",
    "    \"yang\",\"di\",\"ke\",\"dari\",\"dan\",\"dengan\",\"untuk\",\"sebagai\",\"atau\",\"pada\",\"ini\",\n",
    "    \"itu\",\"sangat\",\"ada\",\"kawasan\",\"kota\",\"kabupaten\",\"bantul\",\"yogyakarta\",\n",
    "    \"yogya\",\"jogja\",\"yg\"\n",
    "])\n",
    "\n",
    "def preprocess_text(text, extra_stopwords=None):\n",
    "    text = (text or \"\").lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+|\\S+@\\S+', ' ', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = [t for t in text.split() if len(t)>1]\n",
    "    stopwords = DEFAULT_STOPWORDS.copy()\n",
    "    if extra_stopwords:\n",
    "        stopwords |= set(extra_stopwords)\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    # stemming\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# create preprocessed column\n",
    "df['clean'] = df['review_text'].fillna(\"\").apply(preprocess_text)\n",
    "# remove empty rows\n",
    "df = df[df['clean'].str.strip().str.len() > 3].reset_index(drop=True)\n",
    "print(\"After cleaning:\", len(df), \"documents\")\n",
    "df.to_csv(\"bantul_reviews_clean.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ------------------ TF-IDF & Top terms ------------------\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
    "X = tfidf.fit_transform(df['clean'])\n",
    "terms = tfidf.get_feature_names_out()\n",
    "means = np.asarray(X.mean(axis=0)).ravel()\n",
    "top_n = 15\n",
    "top_idx = means.argsort()[::-1][:top_n]\n",
    "top_terms = terms[top_idx]\n",
    "top_scores = means[top_idx]\n",
    "top_df = pd.DataFrame({\"term\":top_terms, \"score\":top_scores})\n",
    "print(\"Top TF-IDF terms:\\n\", top_df)\n",
    "\n",
    "# save tfidf top\n",
    "top_df.to_csv(\"bantul_tfidf_top_terms.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ------------------ WordCloud & bar chart ------------------\n",
    "all_text = \" \".join(df['clean'].tolist())\n",
    "wc = WordCloud(width=1000, height=400, collocations=False, background_color=\"white\").generate(all_text)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"WordCloud - Opini Wisata Bantul\")\n",
    "plt.show()\n",
    "\n",
    "# bar chart\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(top_df['term'][::-1], top_df['score'][::-1])\n",
    "plt.xlabel(\"Mean TF-IDF\")\n",
    "plt.title(\"Top TF-IDF terms - Bantul (Top {})\".format(top_n))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save final dataframe and results\n",
    "df.to_csv(\"bantul_reviews_final.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved cleaned and final results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d19e1-dc52-43ca-aef4-7a6b68cd3e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
