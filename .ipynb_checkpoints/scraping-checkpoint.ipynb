{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f0ead3c-7c6c-4ea1-a4df-3485e6d2526b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TooManyRequests",
     "evalue": "429 Too Many Requests\nToo Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTooManyRequests\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tweets\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# ------------------ Main: Collect data ------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m docs = \u001b[43mscrape_twitter_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQUERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_TWEETS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m df = pd.DataFrame(docs)\n\u001b[32m     48\u001b[39m df.to_csv(OUTPUT_RAW, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8-sig\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mscrape_twitter_api\u001b[39m\u001b[34m(query, max_tweets)\u001b[39m\n\u001b[32m     24\u001b[39m client = tweepy.Client(bearer_token=\u001b[33m\"\u001b[39m\u001b[33mAAAAAAAAAAAAAAAAAAAAAG205QEAAAAALf7jXB7mo1d5uYkdwRElM2sk\u001b[39m\u001b[33m%\u001b[39m\u001b[33m2Bpo\u001b[39m\u001b[33m%\u001b[39m\u001b[33m3DXjpv9ACBwRCsw0UQhJfcelYwc0GfjIouYewYu0n0bdUC8EAMvf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m tweets = []\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtweepy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPaginator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_recent_tweets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtweet_fields\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcreated_at\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlang\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tweets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43min\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\kuliah\\Semester 5\\nlp\\proyek_akhir\\venv311\\Lib\\site-packages\\tweepy\\pagination.py:67\u001b[39m, in \u001b[36mPaginator.flatten\u001b[39m\u001b[34m(self, limit)\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     66\u001b[39m count = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPaginationIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\kuliah\\Semester 5\\nlp\\proyek_akhir\\venv311\\Lib\\site-packages\\tweepy\\pagination.py:126\u001b[39m, in \u001b[36mPaginationIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m.kwargs[\u001b[33m\"\u001b[39m\u001b[33mpagination_token\u001b[39m\u001b[33m\"\u001b[39m] = pagination_token\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, Response):\n\u001b[32m    129\u001b[39m     meta = response.meta\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\kuliah\\Semester 5\\nlp\\proyek_akhir\\venv311\\Lib\\site-packages\\tweepy\\client.py:1276\u001b[39m, in \u001b[36mClient.search_recent_tweets\u001b[39m\u001b[34m(self, query, user_auth, **params)\u001b[39m\n\u001b[32m   1184\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"search_recent_tweets( \\\u001b[39;00m\n\u001b[32m   1185\u001b[39m \u001b[33;03m    query, *, end_time=None, expansions=None, max_results=None, \\\u001b[39;00m\n\u001b[32m   1186\u001b[39m \u001b[33;03m    media_fields=None, next_token=None, place_fields=None, \\\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1273\u001b[39m \u001b[33;03m.. _Academic Research Project: https://developer.twitter.com/en/docs/projects\u001b[39;00m\n\u001b[32m   1274\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1275\u001b[39m params[\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m] = query\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/2/tweets/search/recent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mend_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexpansions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmedia.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnext_token\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplace.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpoll.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1281\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msince_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msort_order\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstart_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtweet.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muntil_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser.fields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTweet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_auth\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\kuliah\\Semester 5\\nlp\\proyek_akhir\\venv311\\Lib\\site-packages\\tweepy\\client.py:129\u001b[39m, in \u001b[36mBaseClient._make_request\u001b[39m\u001b[34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_request\u001b[39m(\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m, method, route, params={}, endpoint_parameters=(), json=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    125\u001b[39m     data_type=\u001b[38;5;28;01mNone\u001b[39;00m, user_auth=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    126\u001b[39m ):\n\u001b[32m    127\u001b[39m     request_params = \u001b[38;5;28mself\u001b[39m._process_params(params, endpoint_parameters)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_auth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_type \u001b[38;5;129;01mis\u001b[39;00m requests.Response:\n\u001b[32m    133\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\kuliah\\Semester 5\\nlp\\proyek_akhir\\venv311\\Lib\\site-packages\\tweepy\\client.py:115\u001b[39m, in \u001b[36mBaseClient.request\u001b[39m\u001b[34m(self, method, route, params, json, user_auth)\u001b[39m\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(method, route, params, json, user_auth)\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests(response)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code >= \u001b[32m500\u001b[39m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TwitterServerError(response)\n",
      "\u001b[31mTooManyRequests\u001b[39m: 429 Too Many Requests\nToo Many Requests"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Analisis Opini Publik tentang Wisata Bantul (Twitter)\n",
    "# =============================================\n",
    "\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tweepy\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ------------------ Settings ------------------\n",
    "MAX_TWEETS = 1        # batas tweet\n",
    "QUERY = \"(wisata bantul OR parangtritis OR hutan pinus OR mangunan OR parangkusumo OR pantai bantul) lang:id\"\n",
    "OUTPUT_RAW = \"bantul_tweets_raw.csv\"\n",
    "\n",
    "# ------------------ Scraping dari Twitter ------------------\n",
    "def scrape_twitter_api(query, max_tweets=1):\n",
    "    client = tweepy.Client(bearer_token=\"AAAAAAAAAAAAAAAAAAAAAG205QEAAAAALf7jXB7mo1d5uYkdwRElM2sk%2Bpo%3DXjpv9ACBwRCsw0UQhJfcelYwc0GfjIouYewYu0n0bdUC8EAMvf\")\n",
    "    tweets = []\n",
    "    for tweet in tweepy.Paginator(\n",
    "        client.search_recent_tweets,\n",
    "        query=query,\n",
    "        max_results=1,\n",
    "        tweet_fields=[\"created_at\", \"lang\"],\n",
    "    ).flatten(limit=max_tweets):\n",
    "        time.sleep(3)\n",
    "        if tweet.lang == \"in\":\n",
    "            tweets.append({\n",
    "                \"source\": \"twitter\",\n",
    "                \"url\": f\"https://twitter.com/i/web/status/{tweet.id}\",\n",
    "                \"review_title\": \"\",\n",
    "                \"review_text\": tweet.text,\n",
    "                \"rating\": None,\n",
    "                \"date\": tweet.created_at\n",
    "            })\n",
    "    print(f\"Total tweet diambil: {len(tweets)}\")\n",
    "    return tweets\n",
    "\n",
    "# ------------------ Main: Collect data ------------------\n",
    "docs = scrape_twitter_api(QUERY, MAX_TWEETS)\n",
    "df = pd.DataFrame(docs)\n",
    "df.to_csv(OUTPUT_RAW, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Data mentah disimpan ke:\", OUTPUT_RAW)\n",
    "\n",
    "# ------------------ Preprocessing ------------------\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "DEFAULT_STOPWORDS = set([\n",
    "    \"yang\",\"di\",\"ke\",\"dari\",\"dan\",\"dengan\",\"untuk\",\"sebagai\",\"atau\",\"pada\",\"ini\",\n",
    "    \"itu\",\"sangat\",\"ada\",\"kawasan\",\"kota\",\"kabupaten\",\"bantul\",\"yogyakarta\",\n",
    "    \"yogya\",\"jogja\",\"yg\",\"rt\",\"via\",\"aja\",\"nih\",\"ya\",\"loh\",\"dong\",\"deh\"\n",
    "])\n",
    "\n",
    "def preprocess_text(text, extra_stopwords=None):\n",
    "    text = (text or \"\").lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+|\\S+@\\S+', ' ', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = [t for t in text.split() if len(t)>1]\n",
    "    stopwords = DEFAULT_STOPWORDS.copy()\n",
    "    if extra_stopwords:\n",
    "        stopwords |= set(extra_stopwords)\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['clean'] = df['review_text'].fillna(\"\").apply(preprocess_text)\n",
    "df = df[df['clean'].str.strip().str.len() > 3].reset_index(drop=True)\n",
    "print(\"Setelah pembersihan:\", len(df), \"dokumen\")\n",
    "df.to_csv(\"bantul_tweets_clean.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ------------------ TF-IDF & Top terms ------------------\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
    "X = tfidf.fit_transform(df['clean'])\n",
    "terms = tfidf.get_feature_names_out()\n",
    "means = np.asarray(X.mean(axis=0)).ravel()\n",
    "top_n = 15\n",
    "top_idx = means.argsort()[::-1][:top_n]\n",
    "top_terms = terms[top_idx]\n",
    "top_scores = means[top_idx]\n",
    "top_df = pd.DataFrame({\"term\":top_terms, \"score\":top_scores})\n",
    "print(\"\\nTop TF-IDF terms:\\n\", top_df)\n",
    "\n",
    "top_df.to_csv(\"bantul_tfidf_top_terms.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ------------------ WordCloud & Grafik ------------------\n",
    "all_text = \" \".join(df['clean'].tolist())\n",
    "wc = WordCloud(width=1000, height=400, collocations=False, background_color=\"white\").generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"WordCloud - Opini Wisata Bantul (Twitter)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(top_df['term'][::-1], top_df['score'][::-1])\n",
    "plt.xlabel(\"Mean TF-IDF\")\n",
    "plt.title(\"Top TF-IDF terms - Wisata Bantul (Twitter, Top {})\".format(top_n))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df.to_csv(\"bantul_tweets_final.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Hasil akhir disimpan di bantul_tweets_final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11afeda2-9181-467b-9a66-ddfa5455032d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.9.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from wordcloud) (2.3.4)\n",
      "Requirement already satisfied: pillow in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from wordcloud) (12.0.0)\n",
      "Requirement already satisfied: matplotlib in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from wordcloud) (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from matplotlib->wordcloud) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from matplotlib->wordcloud) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from matplotlib->wordcloud) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from matplotlib->wordcloud) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from matplotlib->wordcloud) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85388878-3e1d-48f1-81ae-057bfb902d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-4.16.0-py3-none-any.whl (98 kB)\n",
      "     ---------------------------------------- 98.8/98.8 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting oauthlib<4,>=3.2.0\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "     -------------------------------------- 160.1/160.1 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from tweepy) (2.32.5)\n",
      "Collecting requests-oauthlib<3,>=1.2.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\kuliah\\semester 5\\nlp\\proyek_akhir\\venv311\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2025.11.12)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.3.1 requests-oauthlib-2.0.0 tweepy-4.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eff7ac-6427-4783-a82e-ca636302f2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
